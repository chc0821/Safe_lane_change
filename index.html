<!-- ---  
layout: page  
title: "DiffColor"  
---  


**DiffColor:** Toward High Fidelity Text-Guided Image Colorization with Diffusion Models  

**Authors:** Jianxin Lin, Peng Xiao, Yijun Wang, Rongju Zhang, Xiangxiang Zeng  

**Abstract:** Recent data-driven image colorization methods have enabled automatic or reference-based colorization, while still suffering from unsatisfactory and inaccurate object-level color control. To address these issues, we propose a new method called DiffColor that leverages the power of pre-trained diffusion models to recover vivid colors conditioned on a prompt text, without any additional inputs. DiffColor mainly contains two stages: colorization with generative color prior and in-context controllable colorization. Specifically, we first fine-tune a pre-trained text-to-image model to generate colorized images using a CLIP-based contrastive loss. Then we try to obtain an optimized text embedding aligning the colorized image and the text prompt, and a fine-tuned diffusion model enabling high-quality image reconstruction. Our method can produce vivid and diverse colors with a few iterations, and keep the structure and background intact while having colors well-aligned with the target language guidance. Moreover, our method allows for in-context colorization, i.e., producing different colorization results by modifying prompt texts without any fine-tuning, and can achieve object-level controllable colorization results. Extensive experiments and user studies demonstrate that DiffColor outperforms previous works in terms of visual quality, color fidelity, and diversity of colorization options.  

**<center>Method</center>**

<figure style="text-align: center;">  
  <img src="assets/images/framework-v4-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Framework of Diffcolor. DiffColor mainly contains two stages: 1) colorization with generative color prior that produces accurate and vivid
colorization results; 2) in-context controllable colorization that edits the color of the first-stage output in a way that satisfies the target text prompt.</figcaption>  
</figure>  

**<center>Colorization</center>**


<figure style="text-align: center;">  
  <img src="assets/images/new_main-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Different color prompts applied to the same gray image.</figcaption>  
</figure>  

**<center>Comparison</center>**


<figure style="text-align: center;">  
  <img src="assets/images/new_prompt-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Comparison with other existing text-conditioned image colorization method.</figcaption>  
</figure>   -->

<!DOCTYPE html>  
<html lang="en">  
<head>  
    <meta charset="UTF-8">  
    <meta name="viewport" content="width=device-width, initial-scale=1.0">  
    <title>VividTalker</title>  
    <style>  
        body {  
            font-family: Arial, sans-serif;  
            line-height: 1.6;  
            max-width: 800px;  
            margin: 0 auto;  
            padding: 20px;  
        }  
          
        h1 {  
            font-size: 28px;  
            margin-bottom: 10px;  
            text-align: center;  
        }  
  
        h2 {  
            font-size: 24px;  
            margin-bottom: 10px;  
            text-align: center;  
        }  
          
        figure {  
            margin: 30px 0;  
            text-align: center;  
        }  
          
        img {  
            max-width: 100%;  
            height: auto;  
        }  
          
        figcaption {  
            font-size: 14px;  
            margin-top: 10px;  
        }  
          
        .authors {  
            font-size: 16px;  
            font-weight: bold;  
            text-align: center;  
            margin-bottom: 20px;  
        }  
        /* 设置视频容器 */
        .video-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-around;
        }

        /* 设置每个视频块 */
        .video-block {
            width: 30%; /* 调整视频块的宽度，以适应屏幕大小 */
            margin-bottom: 20px;
        }

        /* 设置标题样式 */
        .video-title {
            font-size: 14px;
            font-weight: bold;
            margin-top: 5px;
            text-align: center;
        }
      .center-content {
            text-align: center;
        }
      
    </style>  
</head>  
<body>  
  
    <h1>Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape</h1>  
  
    <p class="authors">Wei Zhao, Yijun Wang, Tianyu He, Lianying Yin, Jianxin Lin, Xin Jin</p>  
 <div class="center-content">
    <a href="https://github.com/weizhaoMolecules/VividTalker_implement.git" target="_blank" rel="noopener noreferrer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"></path></svg>
      Code
  </a>

  
  <a href="https://arxiv.org/abs/2310.20240" target="_blank" rel="noopener noreferrer">
    <!-- 这里需要替换为适合你项目的 ArXiv 图标的实际路径 -->
    <img src="pdf-icon.png" alt="ArXiv Icon" width="16" height="16">
    Paper
</a>
   </div>
    <h2>Abstract</h2>
    <p class="abstract">The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2)  Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement and encode them separately into discrete latent spaces. Then, these attributes are generated through an autoregressive process leveraging a window-based Transformer architecture. To augment the richness of 3D facial animation, we construct a new 4D dataset with detailed shapes and learn to synthesize facial details in line with speech content. Extensive quantitative and qualitative experiments demonstrate that VividTalker outperforms state-of-the-art methods, resulting in vivid and realistic speech-driven 3D facial animation.  </p>
    <h2>Method</h2>  
  
    <figure>  
        <img src="assets/images/architecture_v2.png" alt="Image description">  
        <figcaption>Overall pipeline of the proposed VividTalker. Our method is composed of two core components: 1) the factor disentanglement module utilizes two VQ-VAE models to encode the head pose and mouth movement into separate discrete latent spaces; 2) the detail enrichment module employs a window-based Transformer to predict motion dynamics (including facial details) over the learned discrete latent space, given an audio signal.
        </figcaption>  
    </figure>  
  <h2>Comparison with baselines and real sample</h2>

<!-- 第一排视频 -->
<div class="video-container">
    <div class="video-block">
        <iframe width="100%" height="200" src="meshtalk.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">MeshTalk</div>
    </div>
    <div class="video-block">
        <iframe width="100%" height="200" src="faceformer.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">FaceFormer </div>
    </div>
    <div class="video-block">
        <iframe width="100%" height="200" src="codetalker.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">CodeTalker</div>
    </div>
</div>

<!-- 第二排视频 -->
<div class="video-container">
    <div class="video-block">
        <iframe width="100%" height="200" src="sadtalker.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">SadTalker</div>
    </div>
    <div class="video-block">
        <iframe width="100%" height="200" src="vividtalker.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">VividTalker(Ours)</div>
    </div>
    <div class="video-block">
        <iframe width="100%" height="200" src="realsample.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">RealSample</div>
    </div>
</div>
</body>  
</html>  
