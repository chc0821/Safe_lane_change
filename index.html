<!-- ---  
layout: page  
title: "DiffColor"  
---  


**DiffColor:** Toward High Fidelity Text-Guided Image Colorization with Diffusion Models  

**Authors:** Jianxin Lin, Peng Xiao, Yijun Wang, Rongju Zhang, Xiangxiang Zeng  

**Abstract:** Recent data-driven image colorization methods have enabled automatic or reference-based colorization, while still suffering from unsatisfactory and inaccurate object-level color control. To address these issues, we propose a new method called DiffColor that leverages the power of pre-trained diffusion models to recover vivid colors conditioned on a prompt text, without any additional inputs. DiffColor mainly contains two stages: colorization with generative color prior and in-context controllable colorization. Specifically, we first fine-tune a pre-trained text-to-image model to generate colorized images using a CLIP-based contrastive loss. Then we try to obtain an optimized text embedding aligning the colorized image and the text prompt, and a fine-tuned diffusion model enabling high-quality image reconstruction. Our method can produce vivid and diverse colors with a few iterations, and keep the structure and background intact while having colors well-aligned with the target language guidance. Moreover, our method allows for in-context colorization, i.e., producing different colorization results by modifying prompt texts without any fine-tuning, and can achieve object-level controllable colorization results. Extensive experiments and user studies demonstrate that DiffColor outperforms previous works in terms of visual quality, color fidelity, and diversity of colorization options.  

**<center>Method</center>**

<figure style="text-align: center;">  
  <img src="assets/images/framework-v4-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Framework of Diffcolor. DiffColor mainly contains two stages: 1) colorization with generative color prior that produces accurate and vivid
colorization results; 2) in-context controllable colorization that edits the color of the first-stage output in a way that satisfies the target text prompt.</figcaption>  
</figure>  

**<center>Colorization</center>**


<figure style="text-align: center;">  
  <img src="assets/images/new_main-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Different color prompts applied to the same gray image.</figcaption>  
</figure>  

**<center>Comparison</center>**


<figure style="text-align: center;">  
  <img src="assets/images/new_prompt-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Comparison with other existing text-conditioned image colorization method.</figcaption>  
</figure>   -->

<!DOCTYPE html>  
<html lang="en">  
<head>  
    <meta charset="UTF-8">  
    <meta name="viewport" content="width=device-width, initial-scale=1.0">  
    <title>VividTalker</title>  
    <style>  
        body {  
            font-family: Arial, sans-serif;  
            line-height: 1.6;  
            max-width: 800px;  
            margin: 0 auto;  
            padding: 20px;  
        }  
          
        h1 {  
            font-size: 28px;  
            margin-bottom: 10px;  
            text-align: center;  
        }  
  
        h2 {  
            font-size: 24px;  
            margin-bottom: 10px;  
            text-align: center;  
        }  
          
        figure {  
            margin: 30px 0;  
            text-align: center;  
        }  
          
        img {  
            max-width: 100%;  
            height: auto;  
        }  
          
        figcaption {  
            font-size: 14px;  
            margin-top: 10px;  
        }  
          
        .authors {  
            font-size: 16px;  
            font-weight: bold;  
            text-align: center;  
            margin-bottom: 20px;  
        }  
        /* 设置视频容器 */
        .video-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-around;
        }

        /* 设置每个视频块 */
        .video-block {
            width: 30%; /* 调整视频块的宽度，以适应屏幕大小 */
            margin-bottom: 20px;
        }

        /* 设置标题样式 */
        .video-title {
            font-size: 14px;
            font-weight: bold;
            margin-top: 5px;
            text-align: center;
        }
      .center-content {
            text-align: center;
        }
      
    </style>  
</head>  
<body>  
  
    <h1>Dynamic Control Authority Allocation in Indirect Shared Control for Steering Assistance</h1>  
  

</a>
   </div>
    <h2>Abstract</h2>
    <p class="abstract">The concept of shared control has garnered significant attention within the realm of human-machine hybrid intelligence research. This study introduces a novel approach, specifically a dynamic control authority allocation method, for implementing shared control in autonomous vehicles. Unlike conventional mixed-initiative control techniques that blend human and vehicle inputs with weights determined by predefined index, the proposed method utilizes optimization-based techniques to obtain an optimal dynamic allocation for human and vehicle inputs that satisfies safety constraints. Specifically, a convex quadratic programm (QP) is constructed incorporating control barrier functions (CBF) for safety and control Lyapunov functions (CLF) for satisfying automated control objectives. The cost function of the QP is designed such that human weight increases with the magnitude of human input. A smooth control authority transition is obtained by optimizing over the change rate of the weight instead of the weight itself. The proposed method is verified in typical lane changing scenarios with human-in-the-loop (HmIL) and hardware-in-the-loop (HdIL) experiments. Results show that the proposed method outperforms index-based control authority allocation method in terms of agility, safety and comfort.  </p>
    <h2>Method</h2>  
  
    <figure>  
        <img src="assets/images/architecture_v2.png" alt="Image description">  
        <figcaption>Overall pipeline of the proposed VividTalker. Our method is composed of two core components: 1) the factor disentanglement module utilizes two VQ-VAE models to encode the head pose and mouth movement into separate discrete latent spaces; 2) the detail enrichment module employs a window-based Transformer to predict motion dynamics (including facial details) over the learned discrete latent space, given an audio signal.
        </figcaption>  
    </figure>  
  <h2>Comparison with baselines and real sample</h2>

<!-- 第一排视频 -->
<div class="video-container">
    <div class="video-block">
        <iframe width="100%" height="200" src="meshtalk.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">MeshTalk</div>
    </div>
    <div class="video-block">
        <iframe width="100%" height="200" src="faceformer.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">FaceFormer </div>
    </div>
    <div class="video-block">
        <iframe width="100%" height="200" src="codetalker.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">CodeTalker</div>
    </div>
</div>

<!-- 第二排视频 -->
<div class="video-container">
    <div class="video-block">
        <iframe width="100%" height="200" src="sadtalker.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">SadTalker</div>
    </div>
    <div class="video-block">
        <iframe width="100%" height="200" src="vividtalker.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">VividTalker(Ours)</div>
    </div>
    <div class="video-block">
        <iframe width="100%" height="200" src="realsample.mp4" frameborder="0" allowfullscreen></iframe>
        <div class="video-title">RealSample</div>
    </div>
</div>
</body>  
</html>  
